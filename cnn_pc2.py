"""
ì‹¤ì‹œê°„ ì°¨ëŸ‰ ì†Œë¦¬ ë¶„ë¥˜ê¸° (PyTorch ë²„ì „)
í•™ìŠµëœ VehicleSoundCNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import warnings
import numpy as np
import pandas as pd
import pyaudio
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import torchaudio.transforms as T
from collections import deque

warnings.filterwarnings('ignore')


# ========================= ì„¤ì • =========================
class Config:
    """ëª¨ë¸ ì„¤ì • (í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€)"""
    # ì˜¤ë””ì˜¤ ì„¤ì •
    SAMPLE_RATE = 22050
    AUDIO_DURATION = 4.0
    N_MELS = 128
    N_FFT = 2048
    HOP_LENGTH = 512
    F_MIN = 20
    F_MAX = 8000

    # ìŠ¤í™íŠ¸ë¡œê·¸ë¨ í¬ê¸°
    SPEC_HEIGHT = 224
    SPEC_WIDTH = 224

    # ëª¨ë¸ ê²½ë¡œ
    MODEL_PATH = './saved_models/vehicle_audio_classifier_fsd50k_best.pth'

    # í´ë˜ìŠ¤ ì´ë¦„
    CLASS_NAMES = {
        0: "air_conditioner",
        1: "car_horn",
        2: "children_playing",
        3: "dog_bark",
        4: "drilling",
        5: "engine_idling",
        6: "gun_shot",
        7: "jackhammer",
        8: "siren",
        9: "street_music"
    }

    # í•œê¸€ í´ë˜ìŠ¤ ì´ë¦„
    CLASS_NAMES_KR = {
        0: "ì—ì–´ì»¨",
        1: "ìë™ì°¨ ê²½ì ",
        2: "ì•„ì´ë“¤ ë…¸ëŠ” ì†Œë¦¬",
        3: "ê°œ ì§–ëŠ” ì†Œë¦¬",
        4: "ë“œë¦´ ì†Œë¦¬",
        5: "ì—”ì§„ ê³µíšŒì „",
        6: "ì´ì†Œë¦¬",
        7: "ì°©ì•”ê¸°",
        8: "ì‚¬ì´ë Œ",
        9: "ê±°ë¦¬ ìŒì•…"
    }

    # íƒ€ê²Ÿ ë¼ë²¨ (ì°¨ëŸ‰ ê´€ë ¨ ì†Œë¦¬)
    TARGET_LABELS = [1, 5, 8]  # car_horn, engine_idling, siren

    # ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì„¤ì •
    CHUNK = 1024
    PYAUDIO_FORMAT = pyaudio.paFloat32
    CHANNELS = 1
    PYAUDIO_RATE = 44100

    # ì˜ˆì¸¡ ì„ê³„ê°’
    CONFIDENCE_THRESHOLD = 0.3  # 30% ì´ìƒì¼ ë•Œë§Œ ì¶œë ¥


config = Config()

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# ========================= ëª¨ë¸ ì •ì˜ =========================
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.se = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(out_channels, out_channels // 16, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels // 16, out_channels, 1),
            nn.Sigmoid()
        )

        self.skip = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.skip = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride),
                nn.BatchNorm2d(out_channels)
            )

        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))

        se_weight = self.se(out)
        out = out * se_weight

        out += self.skip(x)
        out = self.relu(out)
        return out


class VehicleSoundCNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 64, 7, 2, 3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1)
        )

        self.layer1 = self._make_layer(64, 64, 2)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.layer4 = self._make_layer(256, 512, 2, stride=2)

        self.global_pool = nn.AdaptiveAvgPool2d(1)

        self.vehicle_branch = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, 3)
        )

        self.classifier = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )

    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride))
        for _ in range(1, blocks):
            layers.append(ResidualBlock(out_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.global_pool(x)
        features = x.view(x.size(0), -1)

        output = self.classifier(features)
        return output


# ========================= ì „ì²˜ë¦¬ í´ë˜ìŠ¤ =========================
class AudioPreprocessor:
    """ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (í•™ìŠµ ì‹œì™€ ë™ì¼)"""

    def __init__(self, config):
        self.config = config

        # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë³€í™˜ê¸°
        self.mel_transform = T.MelSpectrogram(
            sample_rate=config.SAMPLE_RATE,
            n_fft=config.N_FFT,
            hop_length=config.HOP_LENGTH,
            n_mels=config.N_MELS,
            f_min=config.F_MIN,
            f_max=config.F_MAX
        )

        # ë¦¬ìƒ˜í”ŒëŸ¬ (44100 -> 22050)
        self.resampler = T.Resample(
            orig_freq=config.PYAUDIO_RATE,
            new_freq=config.SAMPLE_RATE
        )

    def preprocess(self, audio_data):
        """
        ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Args:
            audio_data: numpy array (raw audio)

        Returns:
            torch.Tensor: (1, 3, 224, 224) í¬ê¸°ì˜ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨
        """
        # numpy -> torch
        waveform = torch.from_numpy(audio_data).float().unsqueeze(0)

        # ë¦¬ìƒ˜í”Œë§ (44100 -> 22050)
        waveform = self.resampler(waveform)

        # ê¸¸ì´ ì¡°ì • (4ì´ˆ = 88200 ìƒ˜í”Œ)
        target_length = int(self.config.SAMPLE_RATE * self.config.AUDIO_DURATION)

        if waveform.shape[1] > target_length:
            # ì¤‘ì•™ ë¶€ë¶„ ì¶”ì¶œ
            start = (waveform.shape[1] - target_length) // 2
            waveform = waveform[:, start:start + target_length]
        elif waveform.shape[1] < target_length:
            # íŒ¨ë”©
            waveform = F.pad(waveform, (0, target_length - waveform.shape[1]))

        # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±
        mel_spec = self.mel_transform(waveform)

        # dB ìŠ¤ì¼€ì¼ ë³€í™˜
        mel_spec_db = T.AmplitudeToDB()(mel_spec)

        # ì •ê·œí™”
        mean = mel_spec_db.mean()
        std = mel_spec_db.std()
        if std > 0:
            mel_spec_db = (mel_spec_db - mean) / std

        # í¬ê¸° ì¡°ì • (128, X) -> (224, 224)
        mel_spec_db = F.interpolate(
            mel_spec_db.unsqueeze(0),
            size=(self.config.SPEC_HEIGHT, self.config.SPEC_WIDTH),
            mode='bilinear',
            align_corners=False
        ).squeeze(0)

        # 3ì±„ë„ë¡œ ë³µì œ (grayscale -> RGB)
        mel_spec_db = mel_spec_db.repeat(3, 1, 1)

        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        mel_spec_db = mel_spec_db.unsqueeze(0)

        return mel_spec_db


# ========================= ì‹¤ì‹œê°„ ë¶„ë¥˜ê¸° =========================
class RealtimeClassifier:
    """ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë¶„ë¥˜ê¸°"""

    def __init__(self, model_path, config):
        self.config = config
        self.device = device

        # ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
        self.model = self._load_model(model_path)
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: {self.device})")

        # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.preprocessor = AudioPreprocessor(config)

        # ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬ (ìŠ¤ë¬´ë”©ìš©)
        self.prediction_history = deque(maxlen=3)

        # PyAudio ì´ˆê¸°í™”
        self.p = pyaudio.PyAudio()
        self.stream = None

        # í†µê³„
        self.total_predictions = 0
        self.target_detections = 0

    def _load_model(self, model_path):
        """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

        # ëª¨ë¸ ì´ˆê¸°í™”
        model = VehicleSoundCNN(num_classes=10)

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=self.device)

        # state_dict ë¡œë“œ
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"   ğŸ“Š í•™ìŠµ ì •í™•ë„: {checkpoint.get('accuracy', 0):.2f}%")
            print(f"   ğŸš— ì°¨ëŸ‰ ì •í™•ë„: {checkpoint.get('vehicle_accuracy', 0):.2f}%")
        else:
            model.load_state_dict(checkpoint)

        model.to(self.device)
        model.eval()

        return model

    def _collect_audio(self, duration):
        """ì§€ì •ëœ ì‹œê°„ë§Œí¼ ì˜¤ë””ì˜¤ ìˆ˜ì§‘"""
        frames = []
        num_chunks = int(self.config.PYAUDIO_RATE / self.config.CHUNK * duration)

        for _ in range(num_chunks):
            data = self.stream.read(self.config.CHUNK, exception_on_overflow=False)
            frames.append(data)

        # bytes -> numpy array
        audio_data = b''.join(frames)
        audio_array = np.frombuffer(audio_data, dtype=np.float32)

        return audio_array

    def _predict(self, audio_data):
        """ì˜¤ë””ì˜¤ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰"""
        with torch.no_grad():
            # ì „ì²˜ë¦¬
            input_tensor = self.preprocessor.preprocess(audio_data)
            input_tensor = input_tensor.to(self.device)

            # ì˜ˆì¸¡
            output = self.model(input_tensor)

            # Softmaxë¡œ í™•ë¥  ë³€í™˜
            probabilities = F.softmax(output, dim=1)[0].cpu().numpy()

            return probabilities

    def _smooth_predictions(self, probabilities):
        """ì˜ˆì¸¡ ê²°ê³¼ ìŠ¤ë¬´ë”© (ìµœê·¼ 3ê°œ í‰ê· )"""
        self.prediction_history.append(probabilities)

        if len(self.prediction_history) > 0:
            smoothed = np.mean(self.prediction_history, axis=0)
            return smoothed

        return probabilities

    def start(self):
        """ì‹¤ì‹œê°„ ë¶„ë¥˜ ì‹œì‘"""
        print("\n" + "=" * 70)
        print("ğŸ¤ ì‹¤ì‹œê°„ ì°¨ëŸ‰ ì†Œë¦¬ ë¶„ë¥˜ê¸° ì‹œì‘")
        print("=" * 70)
        print(f"ğŸ“Š ìƒ˜í”Œë ˆì´íŠ¸: {self.config.PYAUDIO_RATE} Hz")
        print(f"â±ï¸  ë¶„ì„ ê°„ê²©: {self.config.AUDIO_DURATION:.1f}ì´ˆ")
        print(f"ğŸš— íƒ€ê²Ÿ í´ë˜ìŠ¤: {', '.join([self.config.CLASS_NAMES[i] for i in self.config.TARGET_LABELS])}")
        print(f"ğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’: {self.config.CONFIDENCE_THRESHOLD * 100:.0f}%")
        print("=" * 70)
        print("ğŸ“¢ Ctrl+Cë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”\n")

        # ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹œì‘
        self.stream = self.p.open(
            format=self.config.PYAUDIO_FORMAT,
            channels=self.config.CHANNELS,
            rate=self.config.PYAUDIO_RATE,
            input=True,
            frames_per_buffer=self.config.CHUNK
        )

        try:
            while True:
                # ì˜¤ë””ì˜¤ ìˆ˜ì§‘
                print('ğŸ”Š ì˜¤ë””ì˜¤ ìˆ˜ì§‘ ì¤‘...', end=' ', flush=True)
                audio_data = self._collect_audio(self.config.AUDIO_DURATION)

                # ì˜ˆì¸¡
                probabilities = self._predict(audio_data)

                # ìŠ¤ë¬´ë”©
                smoothed_probs = self._smooth_predictions(probabilities)

                # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤
                predicted_class = np.argmax(smoothed_probs)
                confidence = smoothed_probs[predicted_class]

                self.total_predictions += 1

                # ê²°ê³¼ ì¶œë ¥ (ì„ê³„ê°’ ì´ìƒì¼ ë•Œë§Œ)
                if confidence >= self.config.CONFIDENCE_THRESHOLD:
                    class_name = self.config.CLASS_NAMES[predicted_class]
                    class_name_kr = self.config.CLASS_NAMES_KR[predicted_class]

                    # íƒ€ê²Ÿ í´ë˜ìŠ¤ ê°ì§€ ì‹œ ê°•ì¡°
                    if predicted_class in self.config.TARGET_LABELS:
                        self.target_detections += 1
                        print(f"\nğŸš¨ [DETECTED] {class_name_kr} ({class_name}): {confidence * 100:.1f}%")
                        print(f"   âš ï¸  ì°¨ëŸ‰ ê´€ë ¨ ì†Œë¦¬ ê°ì§€ë¨!")
                    else:
                        print(f"\n   {class_name_kr} ({class_name}): {confidence * 100:.1f}%")
                else:
                    print("(ì‹ ë¢°ë„ ë‚®ìŒ)")

                # ìƒìœ„ 3ê°œ í´ë˜ìŠ¤ ì¶œë ¥
                top3_indices = np.argsort(smoothed_probs)[-3:][::-1]
                print("   [ìƒìœ„ 3ê°œ]", end=" ")
                for idx in top3_indices:
                    print(f"{self.config.CLASS_NAMES_KR[idx]}({smoothed_probs[idx] * 100:.0f}%)", end=" ")
                print()

        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  ì¤‘ì§€ë¨")
        finally:
            self._cleanup()

    def _cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()

        self.p.terminate()

        print("\n" + "=" * 70)
        print("ğŸ“Š í†µê³„")
        print("=" * 70)
        print(f"   ì´ ì˜ˆì¸¡ íšŸìˆ˜: {self.total_predictions}")
        print(f"   ì°¨ëŸ‰ ì†Œë¦¬ ê°ì§€: {self.target_detections}íšŒ")
        if self.total_predictions > 0:
            detection_rate = (self.target_detections / self.total_predictions) * 100
            print(f"   ê°ì§€ìœ¨: {detection_rate:.1f}%")
        print("=" * 70)
        print("âœ… ì¢…ë£Œ ì™„ë£Œ")


# ========================= ë©”ì¸ í•¨ìˆ˜ =========================
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\nğŸš— ì°¨ëŸ‰ ì†Œë¦¬ ì‹¤ì‹œê°„ ë¶„ë¥˜ê¸° (PyTorch ë²„ì „)")
    print(f"ğŸ–¥ï¸  Device: {device}")

    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")

    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    if not os.path.exists(config.MODEL_PATH):
        print(f"\nâŒ ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print(f"   ê²½ë¡œ: {config.MODEL_PATH}")
        print(f"\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print(f"   1. ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµì‹œí‚¤ì„¸ìš”")
        print(f"   2. ë˜ëŠ” MODEL_PATHë¥¼ ì˜¬ë°”ë¥¸ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”")
        return

    try:
        # ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
        classifier = RealtimeClassifier(config.MODEL_PATH, config)
        classifier.start()

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()